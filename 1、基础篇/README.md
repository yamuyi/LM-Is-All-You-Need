# 基础篇：大模型入门基石

## 目录说明

本篇用于存放大模型学习的基础理论、前置知识及入门实践资料，是后续深入学习的核心铺垫。

## 学习目标

1. 理解大模型的核心定义、发展历程及技术演进脉络
2. 掌握深度学习、NLP 基础概念（如注意力机制、Transformer 架构）
3. 搭建基础开发环境，完成首个大模型调用案例
4. 能清晰区分不同类型大模型（基座模型/微调模型/垂类模型）的差异

## 核心内容

| 子目录/文件      | 说明                                                |
| ---------------- | --------------------------------------------------- |
| 01_Introduction  | 大模型发展简史、核心特性及应用场景                  |
| 02_Prerequisites | 深度学习/NLP 前置知识笔记（附公式推导）             |
| 03_Transformer   | Transformer 架构原理解析及代码实现                  |
| 04_First_Demo    | 基础开发环境搭建（Python/conda）及首个 LLM 调用案例 |
| 05_Glossary      | 大模型核心术语词典（中英对照）                      |

## 资源推荐

- **书籍**：《深度学习》（花书）、《自然语言处理入门》
- **论文**：[Attention Is All You Need](https://arxiv.org/abs/1706.03762)（Transformer 奠基论文）
- **课程**：吴恩达《深度学习专项课程》、斯坦福 CS224N
- **工具**：Anaconda（环境管理）、Jupyter Notebook（代码调试）

## 学习建议

1. 先掌握 Transformer 架构细节，这是所有大模型的核心基础
2. 每个理论点配套实践（如用 PyTorch 复现注意力机制）
3. 遇到术语疑问及时查阅 Glossary 并补充记录
