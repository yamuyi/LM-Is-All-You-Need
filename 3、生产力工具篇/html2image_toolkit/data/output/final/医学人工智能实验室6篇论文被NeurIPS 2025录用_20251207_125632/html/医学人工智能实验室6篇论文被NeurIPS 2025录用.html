<html lang="zh-CN">
 <head>
  <meta charset="utf-8"/>
  <title>
   医学人工智能实验室6篇论文被NeurIPS 2025录用
  </title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <style>
   * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        body { 
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "PingFang SC", "Microsoft YaHei", sans-serif; 
            max-width: 1000px;
            margin: 0 auto;
            padding: 40px;
            line-height: 1.8;
            background: white;
            font-size: 18px;  /* 增加基础字体大小 */
            color: #333;
        }
        
        h1 {
            font-size: 32px;  /* 增加h1字体大小 */
            margin: 30px 0 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #eaeaea;
            font-weight: 700;
            color: #222;
        }
        
        h2 {
            font-size: 28px;  /* 增加h2字体大小 */
            margin: 28px 0 18px;
            padding-bottom: 8px;
            border-bottom: 2px solid #f0f0f0;
            font-weight: 600;
            color: #333;
        }
        
        h3 {
            font-size: 24px;  /* 增加h3字体大小 */
            margin: 24px 0 16px;
            font-weight: 600;
            color: #444;
        }
        
        h4 {
            font-size: 20px;  /* 增加h4字体大小 */
            margin: 20px 0 14px;
            font-weight: 600;
            color: #555;
        }
        
        h5, h6 {
            font-size: 18px;  /* 增加h5,h6字体大小 */
            margin: 18px 0 12px;
            font-weight: 600;
            color: #666;
        }
        
        p {
            margin: 16px 0;
            text-align: justify;
            word-wrap: break-word;
            overflow-wrap: break-word;
        }
        
        a {
            color: #0070f3;
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: border-color 0.2s;
        }
        
        a:hover {
            border-bottom-color: #0070f3;
        }
        
        /* 代码块样式 */
        pre {
            background: #f8f8f8;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            overflow-x: auto;
            border: 1px solid #eaeaea;
            font-size: 16px;  /* 代码字体大小 */
        }
        
        code {
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
            background: #f5f5f5;
            padding: 3px 6px;
            border-radius: 4px;
            font-size: 16px;  /* 行内代码字体大小 */
            color: #d63384;
        }
        
        pre code {
            background: transparent;
            padding: 0;
            color: #333;
            font-size: 16px;  /* 代码块内代码字体大小 */
        }
        
        /* 表格样式 */
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
            font-size: 17px;  /* 表格字体大小 */
        }
        
        th, td {
            border: 1px solid #ddd;
            padding: 12px 15px;
            text-align: left;
        }
        
        th {
            background-color: #f8f9fa;
            font-weight: 600;
        }
        
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        
        /* 列表样式 */
        ul, ol {
            margin: 16px 0;
            padding-left: 30px;
        }
        
        li {
            margin: 8px 0;
            line-height: 1.8;
        }
        
        /* 引用块样式 */
        blockquote {
            border-left: 4px solid #0070f3;
            margin: 20px 0;
            padding: 15px 20px;
            background-color: #f9f9f9;
            border-radius: 0 4px 4px 0;
            font-size: 17px;  /* 引用块字体大小 */
        }
        
        blockquote p {
            margin: 0;
        }
        
        /* 图片样式 */
        img {
            display: block;
            margin: 25px auto;
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        /* 目录样式 */
        .toc-container {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 20px;
            margin: 25px 0;
            border: 1px solid #eaeaea;
        }
        
        .toc-title {
            font-size: 18px;
            font-weight: 600;
            margin-bottom: 10px;
            color: #333;
        }
        
        .toc ul {
            list-style-type: none;
            padding-left: 0;
        }
        
        .toc ul ul {
            padding-left: 20px;
        }
        
        .toc li {
            margin: 6px 0;
        }
        
        .toc a {
            color: #555;
            text-decoration: none;
        }
        
        .toc a:hover {
            color: #0070f3;
        }
        
        /* 分割线 */
        hr {
            border: none;
            border-top: 2px solid #eaeaea;
            margin: 30px 0;
        }
        
        /* 脚注 */
        .footnote {
            font-size: 14px;
            color: #666;
        }
        
        /* 内容包装器 */
        .content-wrapper {
            max-width: 100%;
            overflow-wrap: break-word;
        }
        
        /* 打印优化 */
        @media print {
            body {
                padding: 0;
                font-size: 16pt;
            }
            
            h1 { font-size: 28pt; }
            h2 { font-size: 24pt; }
            h3 { font-size: 20pt; }
            h4 { font-size: 18pt; }
            
            pre, code {
                font-size: 14pt;
            }
            
            table {
                font-size: 15pt;
            }
        }
  </style>
 </head>
 <body>
  <div class="content-wrapper">
   <p>
    <strong>
     No.1
    </strong>
   </p>
   <p>
    <strong>
     Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens
    </strong>
   </p>
   <p>
    思不思考？从信息论视角探索大型推理模型中的思维效率
   </p>
   <p>
    接收类型：spotlight
   </p>
   <p>
    <strong>
     【科普一下】
    </strong>
   </p>
   <p>
    近两年，像 QwQ-32B、DeepSeek-R1 这样的“大型推理模型”（LRM）都学会了“先想一大段再给答案”，也就是长链式思考（Long CoT）。表面上看，很认真；但你会发现，它们动不动写上千 token，推理又长又啰嗦，还不便宜。那么问题来了：想得越多，真的越好吗？
   </p>
   <p>
    论文把这个问题拆开，从信息论的角度做了一次非常系统的分析。作者将大模型的思考过程看作“带噪声的通信”，把每一段推理当作一次“不确定性压缩”。  为了衡量“推理质量”而非长度，他们提出两个新指标：InfoBias（信息偏差）与 InfoGain（信息增益）。  前者衡量“当前推理有没有离正确路径越来越远”，后者衡量“这一小段推理到底减少了多少不确定性”。洞察非常有趣：正确的答案往往在早期 steps 就确定了，后面的内容多数是重复、兜圈，甚至把原本的正确思路带偏；错误答案的 InfoBias 更高、InfoGain 更快衰减。这说明很多长推理不是“深度思考”，而是典型的“想太多”。
   </p>
   <p>
    基于这些观察，作者提出了一个非常简单却很有效的想法：既然模型在推理中会不断更新自己对答案的信心，那我们能不能让它“自己决定什么时候停下来”？于是，  一个完全无需训练、只在推理阶段生效的策略——Adaptive Think 诞生了  ：模型每写完一段，就评估自己对最终答案的把握程度，一旦“信心足够高”就立刻刹车，不再继续冗长的推理。在 QwQ-32B 上，这一策略在六大推理任务中实现了 平均准确率 +1.10%，token 消耗 -50.80% 的亮眼成绩。这不仅提供了一个全新的解释框架，也为“推理更高效的大模型”迈出了关键一步。
   </p>
   <p>
    <strong>
     【技术介绍】
    </strong>
   </p>
   <p>
    技术上，这篇论文的核心是把  LRM 的推理看成一个“信息传递过程”  ：题目是信息源，模型不断生成推理步骤，相当于逐步压缩不确定性。作者基于 Shannon 熵和互信息，构建了两个量化指标。  InfoGain 衡量每个推理步骤带来的不确定性减少量  ，可近似理解为“这一段到底有没有实质性贡献”；  InfoBias 则衡量当前推理路径与人类标注的“理想轨迹”偏离程度  ，用互信息差值估计并做 token 归一化，使不同长度的推理可公平对比。为了避免“题目难度”这个混杂因素，他们选择难度更均匀的 GSM8K 做主要分析，并在审稿人建议下额外加入对 MATH500 各难度层的 InfoBias 分布计算。结果显示：即使在同一难度区间，正确 vs 错误样本的 InfoBias 分布仍有稳定差异，证明指标真实刻画的是“推理效率”而非“题目太难”。
   </p>
   <p>
    基于这些信息论观察，他们提出了  Adaptive Think ——一个完全无需训练、实时监控模型思考状态的推理策略  。其机制如下：
   </p>
   <p>
    1.推理切分方式
   </p>
   <p>
    并非按固定 token 长度切块，而是以自然语言段落（语义完成度更高）作为推理 step，使每次评估都对齐一个完整逻辑单元，比“每 32 token probe 一次”的方法更稳定。
   </p>
   <p>
    2.预测熵的估计
   </p>
   <p>
    多选题：直接对选项概率分布求熵；
   </p>
   <p>
    开放式答案：用小规模 beam search / 树搜索构造有限候选集，然后对其概率分布求熵。
   </p>
   <p>
    3.停止条件
   </p>
   <p>
    给定阈值系数 a，当当前熵下降到初始熵的一定比例以下，即视为“确定度够高”，提前中止推理。实验中，调节 a 能在不同任务间找到最佳“准确率–token”平衡。
   </p>
   <p>
    4.额外开销分析
   </p>
   <p>
    “省 token 但会更慢？”。作者给出计算，一次熵计算的标量代价远小于一次 token 解码；在开放式任务中只要最终 token 减少率 &gt; 30%，总体延迟仍下降。实际实验中 GSM8K/AIME2025 的平均减少率接近 60%，支持其结论。
   </p>
   <p>
    需要强调的是，Adaptive Think 的主要适用场景是答案空间有限的推理任务。对于开放式长文本生成，由于难以定义“答案空间”与“理想轨迹”，方法仍有局限。但在当前最主流的数学、逻辑与知识推理任务上，它同时提升了推理质量与推理效率，  为后续“可控思考深度的大模型”研究提供了一个非常有价值的基线方法。
   </p>
   <p>
    <img alt="图片" src="images/医学人工智能实验室6篇论文被NeurIPS 2025录用_51ba560855b80b2988dad24cf8075106.jpg" style="display: block; margin: 25px auto; max-width: 100%; height: auto; border-radius: 4px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);"/>
   </p>
   <p>
    paper地址：https://drive.google.com/file/d/12J6Op2N-vkPm7fS077KQaiJIuv9lD4Xq/view?usp=drive_link
   </p>
   <p>
    项目地址：https://github.com/chicosirius/think-or-not
   </p>
   <p>
    <strong>
     No.2
    </strong>
   </p>
   <p>
    <strong>
     D-VST: Diffusion Transformer for Pathology-Correct Tone-Controllable Cross-Dye Virtual Staining of Whole Slide Images
    </strong>
   </p>
   <p>
    D-VST：面向病理一致与色调可控的跨染色全视野切片虚拟染色扩散 Transformer
   </p>
   <p>
    传统病理诊断依赖不同化学染色（如 HE、IHC、mIHC）来呈现组织结构和分子标志，但每种染色都要额外切片、上机和耗材，既费时又受样本量限制。近年来“虚拟染色”尝试用深度学习从一种染色预测另一种染色，但现有方法往往无法精细控制色调，还可能把参考图像中的病理状态“泄漏”到结果里；同时，面对分辨率极高的全视野切片时，简单切块重建又容易产生明显的马赛克伪影。
   </p>
   <p>
    D-VST 提出一种基于扩散 Transformer 的跨染色虚拟染色框架，目标是实现“病理正确、色调可控”的全视野切片重染色。  模型在 VAE 潜空间中进行扩散去噪，并引入双通路条件编码：一条 pathology encoder 从源域图像（如 HE）中提取组织结构和病理信息，与噪声潜变量在空间上拼接，确保生成图像的病理状态完全由源图决定；另一条 tone encoder 采用 CLIP ViT，对目标染色的参考图像先高斯模糊以抹除细节，仅保留大尺度颜色分布，再编码为全局色调向量，通过多头交叉注意力注入 DiT 各层，从而只调节色彩风格。  训练上使用两阶段 curriculum  ：第一阶段仅学习从源图恢复目标染色结构，第二阶段在固定主体网络的前提下逐步加入模糊的 tone 条件并随机 dropout，使模型被迫将“结构/病理”和“色调”解耦，抑制病理泄漏。针对 WSI，D-VST 还设计频率感知的自适应 patch 采样，在低频区域增加重叠采样以平滑拼接，同时控制计算成本。
   </p>
   <p>
    <img alt="图片" src="images/医学人工智能实验室6篇论文被NeurIPS 2025录用_f9bd4966be8e324ee37563cad46c7367.jpg" style="display: block; margin: 25px auto; max-width: 100%; height: auto; border-radius: 4px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);"/>
   </p>
   <p>
    paper地址：https://drive.google.com/file/d/1LPBMRCdrvUMTIshQMrsMbUErHTyHtUTF/view?usp=drive_link
   </p>
   <p>
    <strong>
     No.3
    </strong>
   </p>
   <p>
    <strong>
     CELLVERSE: Do Large Language Models Really Understand Cell Biology?
    </strong>
   </p>
   <p>
    CELLVERSE：大语言模型真的懂细胞生物学吗？
   </p>
   <p>
    近年来，以GPT为代表的大型语言模型（LLMs）在科学领域的应用潜力备受关注。然而，在细胞生物学这一高度复杂的领域，LLMs是否具备真正的理解与推理能力，而非简单的模式匹配，一直缺乏系统性的评估标准。这一问题直接关系到我们能否信赖AI在疾病诊断、药物研发等关键应用中的判断。为应对此挑战，本文的研究者们开创性地提出了一个名为 CELLVERSE 的统一基准测试平台。  该平台首次将复杂的单细胞多组学数据转化为自然语言问答（QA）任务，旨在全面、严谨地衡量LLMs在细胞生物学领域的真实能力。  通过对14个业界领先的LLMs进行综合“大考”，研究得出了一个发人深省的核心结论：尽管通用大模型展现出初步的生物学推理潜力，但其整体性能远未达到预期，尤其在药物反应预测等关键任务上，其表现甚至不优于随机猜测。这项工作首次通过大规模实证研究，揭示了当前LLMs在理解细胞生物学方面的显著局限，并为该领域的未来发展奠定了关键的评估基石。
   </p>
   <p>
    我们针对当前单细胞分析范式在统一性、用户友好性和可解释性方面的挑战，提出了一种以语言为中心的分析新范式，  并构建了首个用于评估LLMs细胞生物学理解能力的基准数据集——CELLVERSE。
   </p>
   <p>
    CELLVERSE基准数据集的构建方法：
   </p>
   <p>
    通过cell2sentence (C2S)技术将细胞表达谱编码为基因名序列，并通过基因调控网络 (GRN)将基因间相互作用转化为文本描述，将高维、稀疏的生物数据转化为LLMs可处理的自然语言问题。
   </p>
   <p>
    CELLVERSE基准数据集数据简介：
   </p>
   <p>
    多组学数据：整合了scRNA-seq、CITE-seq、ASAP-seq、scATAC-seq四种主流单细胞数据；多层次任务：涵盖了三个核心生物学问题，形成层次化的评估体系：1.细胞类型注释 (CTA)：细胞层面；2.药物反应预测 (DRP)：药物层面；3.基因扰动分析 (PA)：基因层面
   </p>
   <p>
    主要发现：
   </p>
   <p>
    通过对14个主流LLMs（包括GPT系列、Llama、Qwen、DeepSeek等）的全面评估，我们的研究揭示：
   </p>
   <p>
    1.通用模型优于专用模型：  大型通用模型（如GPT-4、DeepSeek）的推理能力显著优于为单细胞任务专门训练的小模型。
   </p>
   <p>
    2.整体性能亟待提升：  即便是最先进的模型，其准确率也远未达到理想水平。例如，在药物反应预测任务上，最佳模型准确率仅为55%，与随机猜测（50%）无显著差异。
   </p>
   <p>
    3.规模效应存在但非万能：  模型性能与规模正相关，但简单的增加上下文长度或使用少样本学习（Few-shot Learning）并不能稳定提升性能，凸显了单细胞数据噪声带来的挑战。
   </p>
   <p>
    <img alt="图片" src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E" style="display: block; margin: 25px auto; max-width: 100%; height: auto; border-radius: 4px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);"/>
   </p>
   <p>
    paper地址：https://drive.google.com/file/d/1NvAPx-iwm3p4n1rmGptACxVKUaLmrOUY/view?usp=drive_link
   </p>
   <p>
    https://cellverse-cuhk.github.io
   </p>
   <p>
    <strong>
     No.4
    </strong>
   </p>
   <p>
    <strong>
     The Curse of Depth in Large Language Models
    </strong>
   </p>
   <p>
    深度魔咒：为什么大模型越深，反而越“没用”？解开 LLM 深层失效之谜
   </p>
   <p>
    在大模型越做越深的今天，一个意想不到的现象正在悄悄发生：模型的“深层”其实几乎不起作用。无论是 Llama、Qwen 还是 DeepSeek，只要把它们的后 1/3 层剪掉，性能竟然几乎不变。这说明，尽管我们习惯于把“更深”视为“更强”，但现实中许多深层 Transformer 只是机械地把输入原样传下去，几乎没做任何实质运算。研究者把这种现象称为 “深度魔咒”（Curse of Depth）。本研究首次从理论和实证上揭示了这一问题的根源：大模型普遍使用的 Pre-LayerNorm 会让深层的数值方差不断放大，导致梯度逐渐退化成“恒等映射”，于是越深的层越“发呆”。更重要的是，论文给出了一个非常简单的解决方案——  LayerNorm Scaling：只要把越深的 LN 输出乘上越小的系数，就能把深层“叫醒”。  实验显示，这个小改动不仅让深层重新工作，还能显著提升大模型的预训练效果和下游任务表现，是一次极轻量却效果显著的结构性改进。
   </p>
   <p>
    本文系统分析了基于 Pre-LayerNorm 的 Transformer 在深度扩展过程中出现的结构性退化现象。理论推导指出，Pre-LN 会导致 LayerNorm 的输出方差随层数累积放大，最坏情况下呈指数级增长，从而使深层块的 Jacobian 逼近单位矩阵，形成“近似恒等映射”——深层的表示变化量趋近于零，导致表达能力与梯度传播同时崩塌。通过大规模实证（包括 LLaMA、Qwen、DeepSeek 等开放权重模型），论文进一步验证：深层的角度距离极低、可被大量剪枝仍不影响性能、对预测贡献度显著弱于浅层。为抑制方差爆炸，  研究者提出 LayerNorm Scaling（LNS），在第 ℓ 层引入 1/√ℓ 的缩放因子，使方差增长从指数级压至多项式级，从理论上恢复深层的有效性。  实验证明，LNS 在 130M～7B 的模型上均能降低 pre-training perplexity、提升 SFT 表现、增强深层表征多样性，并在 ViT 上同样奏效。该方法无需额外参数、无需调参，可直接替代 Pre-LN，是一种简单而通用的深层修复策略。
   </p>
   <p>
    https://drive.google.com/file/d/1f1maADXoP-NdHt187JE70goWIkcSACNx/view?usp=drive_link
   </p>
   <p>
    https://github.com/lmsdss/LayerNorm-Scaling
   </p>
   <p>
    <strong>
     No.5
    </strong>
   </p>
   <p>
    <strong>
     Degradation-aware Dynamic Schrödinger Bridge for Unpaired Image Restoration
    </strong>
   </p>
   <p>
    基于退化感知动态薛定谔桥的无配对图像修复
   </p>
   <p>
    图像恢复是计算机视觉和机器学习中的一项基础任务，其目标是在多种退化条件（例如模糊、低照度、雾霾等）下，学习清晰图像与退化图像之间的映射关系。然而，大多数现有的图像恢复方法都严重依赖退化与清晰图像成对数据的要求，这在缺乏成对数据的大量真实场景中极大限制了其泛化能力和实际可行性。
   </p>
   <p>
    为了解决这一瓶颈，我们提出了一种用于无配对图像恢复的退化感知动态薛定谔桥。  其核心思想是在清晰图像分布与退化图像分布之间学习一条薛定谔桥，同时显式强调物理退化先验，以减少恢复过程中误差的累积。  为此，我们设计了一种退化感知最优传输（Degradation-aware Optimal Transport, DOT）学习方案。
   </p>
   <p>
    训练一个退化模型去学习逆向的恢复过程尤具挑战性，因为该模型必须在迭代恢复过程的不同阶段均保持适用性。为此，  我们进一步提出了一种具有一致性约束的动态传输（Dynamic Transport with Consistency, DTC）学习目标，用于减少早期迭代中的细节损失，从而不断细化退化模型。
   </p>
   <p>
    我们针对当前图像恢复普遍依赖成对“退化-清晰”图像、难以覆盖复杂真实退化场景的局限，  提出了一种面向无配对图像恢复的退化感知动态薛定谔桥框架 Degradation-aware Dynamic Schrödinger Bridge（DDSB）  ，在“清晰–退化”分布之间学习一条动态最优传输路径，同时显式建模物理退化过程，抑制恢复过程中误差的逐步累积。
   </p>
   <p>
    DDSB 方法设计：
   </p>
   <p>
    退化感知最优传输（Degradation-aware Optimal Transport, DOT）  ：在熵正则最优传输框架上，引入一个可学习的退化模型，对“恢复后的结果再退化”并与原始退化图对齐，从而放大并约束模型在恢复过程中引入的伪影与细节丢失，减少多步迭代传输中的误差积累。
   </p>
   <p>
    动态一致性约束（Dynamic Transport with Consistency, DTC）  ：沿着从退化图到清晰图的整条传输轨迹施加时间加权约束，前期迭代权重更高，一方面缓解早期步骤中过度平滑导致的细节丢失，另一方面利用轨迹末端质量更高的结果反向指导退化模型学习，使其在不同迭代阶段都保持稳定、物理合理的“退化仿真”能力。
   </p>
   <p>
    轻量退化网络作为“约束器”而非“重建器”  ：退化模型仅由三层卷积构成，用于刻画物理退化趋势，对不符合真实退化分布的中间恢复结果进行惩罚，在极低参数量的前提下显著提升恢复的物理一致性与稳定性。
   </p>
   <p>
    数据与任务设置简介  ：
   </p>
   <p>
    多任务无配对图像恢复：  在单一 DDSB 框架下，同时处理四类典型退化任务——雨条去除（Rain200L）、雨滴去除（Raindrop）、去模糊（GoPro）与低照度增强（LOL），训练阶段仅使用“退化图 + 随机采样清晰图”的无配对组合。
   </p>
   <p>
    泛化去雾评估：  在 RESIDE SOTS-indoor 上训练去雾模型，并在 SOTS-outdoor 及真实非均匀雾数据集 NH-HAZE 2 上测试，系统评估合成→真实、室内→室外等跨域场景下的泛化能力。
   </p>
   <p>
    统一多任务无配对恢复性能领先：  在雨条、雨滴、低照度与去模糊四个任务上，DDSB 在 PSNR / SSIM 上均取得当前最优结果，同时在 LPIPS、NIQE 等感知指标上也全面优于现有方法，证明退化感知动态传输在复杂退化场景下更稳健。
   </p>
   <p>
    跨场景去雾泛化能力突出：  仅在室内合成雾数据训练的 DDSB，在室外合成雾和真实非均匀雾数据上PSNR / SSIM优越，显示出对未知退化类型和域偏移的优异适应性。
   </p>
   <p>
    理论可解释性及实用性：  在理论上，从一般化误差界出发证明引入退化感知项可获得更紧的泛化上界；在工程上，DDSB 仅约 14.7M 参数、512×512 分辨率下推理延迟远低于多数对比方法，具备良好的部署效率与实际应用潜力。
   </p>
   <p>
    https://drive.google.com/file/d/1441t4lMsdLgTXLvvinGnb0ttjj8ekgXi/view?usp=drive_link
   </p>
   <p>
    <strong>
     No.6
    </strong>
   </p>
   <p>
    <strong>
     Learning a Cross-Modal Schrödinger Bridge for Visual Domain Generalization
    </strong>
   </p>
   <p>
    学习用于视觉域泛化的跨模态薛定谔桥
   </p>
   <p>
    近年来，以CLIP、EVA-02为代表的视觉–语言基础模型凭借其强大的零样本泛化能力，为视觉域泛化任务带来了新希望，即让AI模型在完全未见过的新环境中依然稳定可靠，比如雾天街道、不同医院的影像设备拍摄的图像等。然而现有方法大多依赖“一步到位”的静态对齐策略，例如通过固定文本提示去匹配图像特征，其核心假设是图像特征可以被直接且可靠地投射到文本语义上。这一假设在面对剧烈的域偏移时常常失效，导致模型容易被虚假线索误导，泛化能力受限。
   </p>
   <p>
    为突破这一瓶颈，  本文提出了SBGen，这是首个将跨模态薛定谔桥引入视觉域泛化的框架。  它的核心思想是  将“图像到文本”的语义对齐过程从僵化的“直线匹配”升级为可学习的随机演化旅程  ：首先由文本引导筛选出图像中语义相关的关键区域，再让这些特征在噪声扰动与语义引导下边走边校准，最终稳健抵达与文本一致的域无关表征。大量实验表明，SBGen在图像分类和分割任务上均达到当前最优性能，尤其在从游戏画面迁移到真实街景的跨域分割中，性能显著领先现有方法。更重要的是，从理论上证明了该随机演化机制具备更紧的泛化误差上界，从根本上优于静态匹配策略。
   </p>
   <p>
    本文提出 SBGen（Schrödinger Bridge for Generalization），一种面向视觉域泛化的跨模态随机对齐新框架，旨在克服现有视觉–语言模型在域泛化中依赖静态匹配（如余弦相似度、prompt微调）所带来的语义断裂与泛化瓶颈。其核心思想是将  “从域偏置图像特征到域无关文本语义”的映射过程，重构为一条受控的随机演化路径  ：不再强求一步到位的硬对齐，而是让特征在语义引导下“边演化边校准”，逐步逼近目标语义。
   </p>
   <p>
    该框架由三个协同模块构成：首先，文本引导的域感知特征筛选依据类别级文本提示（如“行人”“车辆”），从密集视觉特征图中挑选出与语义高度对齐的局部区域，有效抑制光照、纹理等域特异干扰；其次，随机跨域演化将筛选后的特征作为起点，通过一个可学习的时间–语义条件漂移函数驱动其沿带噪声的随机轨迹演化，该过程显式建模了从源域表征向文本锚点过渡的中间状态，增强了对分布偏移的适应能力；最后，随机域不变插值将演化终点的特征写回原始特征图对应位置，生成语义增强后的视觉表征，供下游任务解码器使用。整个流程端到端可微，通过联合优化任务损失（如分类交叉熵或分割mIoU）与演化一致性约束进行训练，后者确保演化路径贴近参考扩散过程，避免过拟合。
   </p>
   <p>
    理论分析表明，  该方法的泛化误差上界严格优于任意确定性映射方案  ，因其通过KL散度控制演化路径，而传统方法依赖总变差距离，后者在数学上更松。实验验证覆盖五大分类基准（PACS、VLCS、OfficeHome、DomainNet、TerraIncognita）与四大分割基准（Cityscapes、BDD100K、Mapillary、GTA5），SBGen在所有设置下均达到当前最优性能；尤其在GTA5→真实街景的跨域分割任务中，平均mIoU达68.74%，显著优于次优方法1.59个百分点；消融研究证实三阶段模块分别带来约0.5%、0.8%、0.6%的性能增益，缺一不可。可视化分析进一步显示，SBGen能有效弥合源域与多个目标域间的特征分布间隙，使不同域样本在语义空间中自然聚拢。  这项工作首次将分布偏移建模为结构化的语义演化过程，为域泛化提供了兼具理论保障与工程可行性的新范式。
   </p>
   <p>
    https://drive.google.com/file/d/1-MoliYlFLdCzfQGmebLIu-ngy8yYD20s/view?usp=drive_link
   </p>
  </div>
 </body>
</html>
