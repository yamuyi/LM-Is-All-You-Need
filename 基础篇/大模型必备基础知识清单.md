# 大模型必备基础知识清单

## 一、Transformer 基础篇

### （一）Transformer 基础

1. Self-Attention 核心原理
2. Layer Norm 与 BN 的差异
3. 位置编码的作用与实现
4. 多头注意力机制的优势
5. Subword Token 的意义
6. BERT 中的 mask 机制
7. BERT 与原生 Transformer 的区别
8. 一词多义的处理方式
9. 残差连接的作用与实现
10. 预训练目标的设计（MLM vs. NSP）

### （二）Attention 计算细节

1. 注意力计算方式与参数量
2. Transformer 模型基本结构
3. 长距离依赖处理机制
4. 缩放点积注意力的重要性
5. 掩码（masking）技术的应用
6. 前馈网络的作用
7. 多头注意力的并行计算逻辑
8. 注意力可视化与分析

### （三）注意力机制

1. 自注意力机制的效率优化方法
2. 多尺度表示问题的解决思路
3. 计算效率与表示能力的权衡
4. Transformer 变体（如 ViT、GAT）的特点
5. 常见损失函数与评价指标的缺陷
6. 冷启动与数据分布不均问题的处理
7. 稀疏注意力机制（Sparse Attention）
8. 跨模态注意力机制

## 二、大模型篇

### （一）大模型基本盘与架构

1. 主流开源模型体系
2. 大模型 LLM 的架构
3. Prefix LM 与 Causal LM 的区别
4. 涌现能力的成因
5. Bert 与 LLaMA、ChatGLM 类大模型的选择依据
6. 混合专家模型（MoE）的原理与应用
7. 模型并行与分布式训练策略

### （二）大模型架构与核心技术

1. 主流大模型架构（BART、T5、GPT、GLM）
2. RLHF 流程与原理
3. LoRA 的实现与参数设置
4. Instruction Tuning 与 Prompt Learning 的差异
5. 全参数微调和高效微调的对比
6. 知识蒸馏（Knowledge Distillation）的实现
7. 上下文学习（In-Context Learning）机制

### （三）大模型训练与微调

1. 预训练与微调的知识注入阶段
2. 领域大模型预训练数据集选择
3. 微调数据集的构建原则（含 SFT、RM、PPO 数据格式）
4. 基座模型（Chat/Base）的选择依据
5. 灾难性遗忘的成因与缓解（含领域训练后通用能力保持）
6. 全参数微调的显存需求
7. 领域模型 Continue PreTrain 的数据选取与知识强化
8. 微调后模型能力劣化的原因
9. 多轮对话任务的微调方法
10. 中文大模型训练经验
11. 混合精度训练（Mixed Precision Training）
12. 数据增广在微调中的应用
13. 模型剪枝（Pruning）与量化（Quantization）

### （四）大模型常见问题

1. 复读机问题的成因与缓解策略
2. LLaMA 输入句子长度的理论限制
3. 长文本处理的优化方案
4. 各专业领域是否需要专属大模型
5. 如何让大模型处理更长的文本
6. 样本量增大导致的 OOM 错误处理
7. 模型偏见与公平性问题
8. 多语言能力优化

### （五）大模型进阶问题

1. 长度外推问题及解决方法
2. 不同专业领域大模型的必要性
3. 大模型输出合规化的实现方式
4. 应用模式变更相关问题
5. 模型可解释性研究
6. 联邦学习与隐私保护

### （六）LangChain 相关

1. LangChain 核心概念（Components、Chains、Prompt Templates 等）
2. 基于 LLM + 向量库的文档对话思路与核心技术
3. 文档对话的 Prompt 模板构建
4. 文档切分粒度的平衡策略
5. LangChain 的功能与使用方法
6. LangChain 中 Embedding 和 Vector Store 的实现
7. LangChain 的低效令牌使用问题解决
8. LangChain 缺乏标准数据类型的处理
9. RAG（Retrieval-Augmented Generation）的原理与优化
10. 工具调用（Tool Calling）与 Agent 设计

### （七）参数高效微调（PEFT）

1. PEFT 的必要性与优点
2. PEFT 与全量微调的区别
3. 微调批处理大小对显存和速度的影响
4. 高效微调技术的问题与最佳实践
5. 多种高效微调方法对比
6. LoRA 系列（LoRA、QLoRA、AdaLoRA）的思路与特点
7. 适配器微调（Adapter-tuning、AdapterFusion 等）的思路
8. 提示学习（Prefix-tuning、Prompt-tuning、P-tuning 等）的区别与优缺点
9. IA3（Infused Adapter by Inhibiting and Amplifying Inner Activations）
10. 微调中的梯度累积（Gradient Accumulation）

### （八）大模型推理

1. 大模型推理时显存占用高的原因
2. GPU 与 CPU 上的推理速度对比
3. int8 与 fp16 推理速度对比
4. 大模型生成时的参数设置
5. 省内存的训练 / 微调 / 推理方法
6. 大模型是否具备推理能力
7. 动态量化与推理加速
8. 批处理推理优化

### （九）大模型评测

1. 模型性能评价指标（EM、F1、ROUGE 等）
2. 大模型的 Honest 原则实现方式
3. 模型对已知 / 未知知识的判断能力训练
4. 领域模型微调的领域评测集构建
5. 自动化评测框架
6. 模型鲁棒性与对抗性测试

### （十）大模型强化学习

1. 奖励模型与基础模型的一致性要求
2. RLHF 实践中的不足
3. 人工偏好数据集的量产问题解决
4. SFT->RM->PPO 训练流程的效率优化
5. PPO 训练的计算资源优化
6. DPO（Direct Preference Optimization）与 RLHF 的对比

### （十一）大模型训练集

1. 微调数据量的需求
2. 大模型训练集的来源
3. 预训练数据 Token 重复对性能的影响
4. 数据去重与清洗策略
5. 合成数据生成技术

### （十二）大模型 Agent

1. 给 LLM 注入领域知识的方法
2. 快速体验各类模型的途径
3. 记忆机制（Memory-Augmented Agents）

### （十三）位置编码与长度外推

1. 位置编码的类型（绝对、相对）
2. 旋转位置编码（RoPE）的思路与应用模型
3. ALiBi 的思路、偏置矩阵作用与应用模型
4. 长度外推问题及解决方法
5. 动态位置编码（Dynamic Positional Encoding）

### （十四）LLMs Tokenizer

1. Byte-Pair Encoding（BPE）的词典构建
2. WordPiece 与 BPE 的异同
3. SentencePiece 的思路
4. 不同大模型的分词方式及区别
5. 领域模型词表扩增的必要性与方法
6. Tokenizer 对多语言支持的影响
7. 动态词表扩展技术

### （十五）Layer Normalization

1. 各 LLM 使用的 Layer Normalization 类型
2. Layer Normalization 在 LLM 中的位置差异
3. Layer Norm 与 RMS Norm 的计算公式及特点
4. Deep Norm 的思路、优点与代码实现
5. Layer Norm 的初始化策略

### （十六）大模型部署与优化

1. 模型压缩技术（Pruning、Quantization、Distillation）
2. 推理框架的选择与优化（ONNX、Triton、vLLM）
3. 云端与边缘部署的差异
4. 模型服务化（API 设计、负载均衡）
5. 实时推理的延迟优化策略

### （十七）大模型安全与伦理

1. 模型攻击与防御（对抗样本、Prompt Injection）
2. 数据隐私保护技术（差分隐私、联邦学习）
3. 模型输出的安全性检测与过滤
4. AI 伦理规范与合规性要求

### （十八）大模型开发工具与生态

1. 主流深度学习框架（PyTorch、TensorFlow、JAX）对比
2. 分布式训练框架（DeepSpeed、Megatron-LM、FSDP）
3. 开源模型生态（Hugging Face、vLLM、Llama.cpp）

### （十九）行业趋势与前沿研究

1. 多模态大模型（Vision-Language、Audio-Text）
2. 自回归模型与扩散模型的融合
3. 通用人工智能（AGI）相关研究进展
4. 低资源语言模型的挑战与解决方案